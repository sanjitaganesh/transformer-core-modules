# Transformer Modules Implementation

This project implements core building blocks of a decoder-only Transformer (GPT-2 style):

- Positional Encoding (sinusoidal method)
- Multi-Head Self-Attention (from scratch using NumPy)

## Deliverables

-Control flow diagram of GPT-2 token processing  
-Positional Encoding module  
-Multi-Head Attention module  

## Files

- `positional_encoding.py` – generates sinusoidal encodings
- `multi_head_attention.py` – standalone implementation of multi-head attention
- `diagram.png` – hand-drawn control-flow diagram

## Requirements
- Python 3.9+
- NumPy

## Author
Sanjita Bhaavya Ganesh
